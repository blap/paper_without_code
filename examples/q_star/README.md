# Code for "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning (Idea behind OpenAI o1)"

Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, Bo An

The paper introduces Q*, a groundbreaking framework aimed at enhancing the multi-step reasoning capabilities of Large Language Models (LLMs) through deliberative planning. Conceptualized as a Markov Decision Process (MDP), Q leverages heuristic search methods like the A* algorithm and plug-and-play Q-value models to guide decision-making without the need for task-specific fine-tuning. This novel approach addresses the limitations of LLMs’ auto-regressive generation, improving accuracy and consistency in complex tasks such as math problem-solving and code generation. Extensive empirical validation on datasets like GSM8K, MATH, and MBPP demonstrates that Q* significantly outperforms existing baselines, setting a new benchmark in the field. The Q* framework has a similar approach to OpenAI’s advanced o1 model and it underscores its practical relevance and effectiveness. Researchers and graduate students will find this paper invaluable due to its robust theoretical grounding, innovative methodology, and strong empirical results. Future research could explore refining heuristic functions, enhancing scalability, and broadening task validation to further optimize and generalize the Q* framework. The detailed and well-structured presentation of complex methodologies makes this paper an essential read for those looking to advance the capabilities of LLMs in multi-step reasoning tasks. Code in this blog is generated by combining OpenAI o1 and Claude Sonnet 3.5.