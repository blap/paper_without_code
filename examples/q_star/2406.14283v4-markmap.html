<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.1-alpha.4/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.1-alpha.4/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:q,mm:v}=window,j=new q.Toolbar;j.attach(v);const we=j.render();we.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(we)})})()</script><script>((f,d,h,u)=>{const g=f();window.mm=g.Markmap.create("svg#mindmap",(d||g.deriveOptions)(u),h)})(()=>window.markmap,null,{"content":"Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning","children":[{"content":"Research Question/Objective","children":[{"content":"Objective","children":[{"content":"To introduce Q*, a novel framework to enhance multi-step reasoning in Large Language Models (LLMs).","children":[],"payload":{"lines":"5,6"}},{"content":"To alleviate errors, hallucinations, and inconsistencies in multi-step reasoning tasks without task-specific fine-tuning.","children":[],"payload":{"lines":"6,8"}}],"payload":{"lines":"4,5"}}],"payload":{"lines":"3,4"}},{"content":"Methodology","children":[{"content":"Q* Framework","children":[{"content":"Formalizes multi-step reasoning as a heuristic search problem.","children":[],"payload":{"lines":"10,11"}},{"content":"Models reasoning steps as a Markov Decision Process (MDP).","children":[],"payload":{"lines":"11,12"}},{"content":"Utilizes A* search algorithm.","children":[],"payload":{"lines":"12,13"}}],"payload":{"lines":"9,10"}},{"content":"Estimation of Optimal Q-Value","children":[{"content":"<strong>Offline Reinforcement Learning:</strong> Uses Fitted Q-iteration to estimate Q-values.","children":[],"payload":{"lines":"14,15"}},{"content":"<strong>Learning from Rollout:</strong> Generates trajectories through rollouts to create Q-value labels.","children":[],"payload":{"lines":"15,16"}},{"content":"<strong>Completion with Stronger LLMs:</strong> Uses advanced models like GPT-4 for better Q-value prediction.","children":[],"payload":{"lines":"16,18"}}],"payload":{"lines":"13,14"}}],"payload":{"lines":"8,9"}},{"content":"Key Findings/Contributions","children":[{"content":"Contributions","children":[{"content":"<em><em>Q</em> Framework:</em>* A general, versatile, and agile framework for multi-step reasoning.","children":[],"payload":{"lines":"20,21"}},{"content":"<strong>MDP Formalization:</strong> Multi-step reasoning modeled as MDP.","children":[],"payload":{"lines":"21,22"}},{"content":"<strong>Heuristic Function:</strong> Developed plug-and-play Q-value models.","children":[],"payload":{"lines":"22,23"}},{"content":"<strong>Empirical Validation:</strong> Demonstrated Q*'s effectiveness on GSM8K, MATH, and MBPP datasets.","children":[],"payload":{"lines":"23,25"}}],"payload":{"lines":"19,20"}}],"payload":{"lines":"18,19"}},{"content":"Theoretical Framework","children":[{"content":"Markov Decision Processes (MDP)","children":[{"content":"States represent partial reasoning steps.","children":[],"payload":{"lines":"27,28"}},{"content":"Actions represent next reasoning steps.","children":[],"payload":{"lines":"28,29"}},{"content":"Rewards measure task completion accuracy.","children":[],"payload":{"lines":"29,30"}}],"payload":{"lines":"26,27"}},{"content":"Heuristic Search Algorithm","children":[{"content":"Uses A* algorithm for optimal step selection.","children":[],"payload":{"lines":"31,32"}},{"content":"Heuristic values guide the search process.","children":[],"payload":{"lines":"32,34"}}],"payload":{"lines":"30,31"}}],"payload":{"lines":"25,26"}},{"content":"Results and Discussion","children":[{"content":"Experimental Results","children":[{"content":"<strong>GSM8K Dataset:</strong> Achieved 80.8% accuracy with Llama-2-7b model.","children":[],"payload":{"lines":"36,37"}},{"content":"<strong>MATH Dataset:</strong> Reached 55.4% accuracy with DeepSeek-Math-7b model.","children":[],"payload":{"lines":"37,38"}},{"content":"<strong>MBPP Dataset:</strong> Achieved 77.0% accuracy with CodeQwen1.5-7b-Chat model.","children":[],"payload":{"lines":"38,40"}}],"payload":{"lines":"35,36"}},{"content":"Comparative Analysis","children":[{"content":"Outperformed Best-of-N and PPO methods.","children":[],"payload":{"lines":"41,42"}},{"content":"Demonstrated significant gains without extensive fine-tuning or laborious prompt engineering.","children":[],"payload":{"lines":"42,44"}}],"payload":{"lines":"40,41"}}],"payload":{"lines":"34,35"}},{"content":"Implications","children":[{"content":"<strong>Practical Deployment:</strong> Validates Q* for real-world tasks like math reasoning and code generation.","children":[],"payload":{"lines":"45,46"}},{"content":"<strong>Avoids Fine-tuning:</strong> Effective without modifying LLM parameters, preserving generality across tasks.","children":[],"payload":{"lines":"46,48"}}],"payload":{"lines":"44,45"}},{"content":"Limitations","children":[{"content":"<strong>Complexity:</strong> Implementation complexity may hinder practical deployment.","children":[],"payload":{"lines":"49,50"}},{"content":"<strong>Heuristic Dependence:</strong> Efficiency reliant on accuracy of heuristic Q-value models.","children":[],"payload":{"lines":"50,52"}}],"payload":{"lines":"48,49"}},{"content":"Future Research Directions","children":[{"content":"<strong>Refinement of Heuristics:</strong> Improve heuristic function accuracy.","children":[],"payload":{"lines":"53,54"}},{"content":"<strong>Scalability:</strong> Enhance framework scalability for larger tasks.","children":[],"payload":{"lines":"54,55"}},{"content":"<strong>Broader Validation:</strong> Apply to diverse reasoning tasks beyond current datasets.","children":[],"payload":{"lines":"55,56"}},{"content":"<strong>Planning Algorithm Integration:</strong> Explore integration with other advanced planning techniques.","children":[],"payload":{"lines":"56,58"}}],"payload":{"lines":"52,53"}},{"content":"Summary","children":[{"content":"Main Contributions","children":[{"content":"Introduces Q* framework for enhanced multi-step reasoning in LLMs.","children":[],"payload":{"lines":"60,61"}},{"content":"Utilizes MDPs and heuristic search for optimal decision-making without fine-tuning.","children":[],"payload":{"lines":"61,62"}}],"payload":{"lines":"59,60"}},{"content":"Supporting Evidence","children":[{"content":"Extensive experiments validate Q* across various tasks.","children":[],"payload":{"lines":"63,64"}},{"content":"Demonstrates significant improvements over existing methods.","children":[],"payload":{"lines":"64,65"}}],"payload":{"lines":"62,63"}},{"content":"Clarity and Presentation","children":[{"content":"Well-structured, clear methodology and results.","children":[],"payload":{"lines":"66,67"}},{"content":"Effective use of figures to illustrate main concepts.","children":[],"payload":{"lines":"67,68"}}],"payload":{"lines":"65,66"}}],"payload":{"lines":"58,59"}}],"payload":{"lines":"1,2"}},{"colorFreezeLevel":5,"initialExpandLevel":-1,"maxWidth":300})</script>
</body>
</html>
