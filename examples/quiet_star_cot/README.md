# OpenAI o1-like CoT code inspired by "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"

Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman

Quiet-STaR represents a significant evolution of the Self-Taught Reasoner (STaR) algorithm, offering advanced capabilities that address key limitations of its predecessor. Unlike STaR, which was confined to specific question-answering datasets, Quiet-STaR generalizes reasoning to unstructured text, enabling language models (LMs) to autonomously generate rationales for each token. This broad applicability is achieved through innovative methodologies, including a tokenwise parallel sampling algorithm that ensures computational efficiency, and the introduction of meta-tokens to seamlessly integrate internal thoughts into the LM’s predictions. Additionally, a mixing head mechanism dynamically balances base model predictions with rationale-based enhancements, mitigating performance dips due to initial out-of-distribution issues.

The potential impact of Quiet-STaR is profound, as it complements chain-of-thought (CoT) prompting by enhancing the depth and coherence of recursive answer generation. This symbiotic relationship underscores Quiet-STaR’s capability to foster deeper thinking in LMs, making it a valuable tool for researchers and practitioners aiming to push the boundaries of artificial intelligence. Furthermore, the efficiency gains and robust performance suggest promising applications in diverse, real-world scenarios, extending beyond the academic benchmarks. Future research can build on these findings by exploring dynamic rationale generation, broader cross-task evaluations, and integrating human feedback, to further close the gap between machine and human-like reasoning. Quiet-STaR’s advancements not only refine the art of reasoning in LMs but also pave the way for more sophisticated and contextually aware AI systems like OpenAI’s recent o1 model. The authors have provided the official implementation on models, and this blog post focus on how to use Quiet-STaR to improve chain-of-thought recursively and simulate OpenAI’s o1 with a simple GPT-4o-mini model.